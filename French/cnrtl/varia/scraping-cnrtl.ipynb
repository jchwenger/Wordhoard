{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction (scraping) de mots du [CNRTL](http://www.cnrtl.fr/definition/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re, urllib3, string, json\n",
    "from time import sleep\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnrtl_url = 'http://www.cnrtl.fr'\n",
    "cnrtl_blurb = '/portailindex/LEXI/'\n",
    "cnrtl_dicts = [\n",
    "                ('cnrtl', 'TLFI'), \n",
    "                ('acad9', 'ACA9'), \n",
    "                ('acad8', 'ACA8'),\n",
    "                ('acad4', 'ACA4'), \n",
    "                ('francophonie', 'FRAN'), \n",
    "                ('bhvf', 'BHVF'), \n",
    "                ('dmf', 'ADMF')\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url):\n",
    "    r = http.request('GET', url)\n",
    "    if r.status == 200: \n",
    "        # clear_output()\n",
    "        print('Joy! Able to scrape', url)\n",
    "        soup = BeautifulSoup(r.data, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        print('Oops, issue:', r.status)\n",
    "        print('Url:', url)\n",
    "        print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(soup):\n",
    "    words = []\n",
    "    for item in soup.find_all('table', class_='hometab')[0].find_all('a'):\n",
    "        words.append(item.text)\n",
    "    words.sort()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_link(soup):\n",
    "    next_page = '/images/portail/right.gif'\n",
    "    link_img = soup.find('img', src=next_page)\n",
    "    if link_img is not None:\n",
    "        new_link = link_img.find_parent()\n",
    "        new_link = cnrtl_url + new_link.get('href')\n",
    "        return new_link\n",
    "    else:\n",
    "        print('Done.')\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(url):\n",
    "    sleep(.5)\n",
    "    soup = get_data(url)\n",
    "    words = get_words(soup)\n",
    "    new_url = next_link(soup)\n",
    "    return words, new_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour scraper tous les dictionnaires (en fait inutile, seuls le cnrtl (0) et le dnf (6) ont des listes d'entrées différentes, les autres sont identiques au cnrtl):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dicts = []\n",
    "\n",
    "for item in [cnrtl_dicts[0], cnrtl_dicts[6]]:\n",
    "    word_list = []\n",
    "    for letter in string.ascii_uppercase[25:]:\n",
    "        url = cnrtl_url + cnrtl_blurb + item[1] + '/' + letter\n",
    "        while url:\n",
    "            words, url = scraper(url)\n",
    "            word_list.extend(words)\n",
    "            \n",
    "    my_dicts.append(word_list)        \n",
    "    print('Number of entries in ', item[0], ':', len(word_list))\n",
    "    \n",
    "    with open(item[0] + '.txt', 'w') as txt:\n",
    "        for word in word_list:\n",
    "            txt.write(word)\n",
    "            txt.write('\\n')\n",
    "        \n",
    "    with open(item[0] + '.json', 'w') as txt:\n",
    "        json.dump(word_list, txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Scraping-CNRTL.ipynb to script\n",
      "[NbConvertApp] Writing 2591 bytes to Scraping-CNRTL.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Scraping-CNRTL.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
